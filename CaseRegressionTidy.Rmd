---
title: "CaseRegressionTidy"
author: "Jessica Starck & LoYu Liu"
date: "3/8/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load Package: 

```{r, warning=FALSE, echo=FALSE}
library(readr)
library(readxl)
library(dplyr)
library(ggplot2)
library(caret)
library(DMwR)
library(nlme)  ## for na.action
library(lubridate) ## for date transformation
```

Load Data: 

```{r}
load("CaseRegressionTidy_2.Rdata")
dim(ended_loans_new)
```

```{r}
## Clean the columns which contain more than 0.5% missing value
ended_loans_new <- ended_loans_new[, colMeans(is.na(ended_loans_new)) < 0.0005]
```

Now, the data dimension become `8102541` rows and `62` variables

```{r}
dim(ended_loans_new)
```

```{r}
## Categorized loan status
## if the loan status was Fully Paid, returns 1, or else, returns 0
ended_loans_new <- ended_loans_new %>%
  mutate(status = ifelse(loan_status == "Fully Paid", 1, 0))

ended_loans_new %>%
  group_by(status) %>%
  summarize(total = n(),
            perc = round((n()/nrow(ended_loans_new))*100, 2))
```

```{r}
ended_loans_new$terms[ended_loans_new$term == "36 months"] <- 36
ended_loans_new$terms[ended_loans_new$term == "60 months"] <- 60
```

```{r}
## Define ROI
ended_loans_new <- ended_loans_new %>%
  mutate(ROI = total_pymnt_inv - funded_amnt_inv)
```

histogram/density plot 

```{r}
## density plot
ggplot(ended_loans_new, aes(x = ROI)) + 
  geom_density() + 
  geom_vline(aes(xintercept = mean(ROI)),
            color = "tomato", 
            linetype = "dashed", 
            size = 1) + 
  ggtitle("Density plot of the Investment Returns") 
```


```{r funded_amnt and ROI}
ggplot(ended_loans_new, mapping = aes(x = funded_amnt, y = ROI)) +
  geom_point(aes(color = loan_status))
```

```{r term ROI}
ggplot(ended_loans_new, mapping = aes(x = term, y = ROI)) +
  geom_boxplot(aes(fill = loan_status))
```

```{r grade ROI}
ggplot(ended_loans_new, mapping = aes(x = as.factor(grade), y = ROI)) +
  geom_boxplot(aes(fill = loan_status)) +
  xlab("grade")
```


```{r}
ended_loan_new_num <- ended_loans_new %>% 
  select_if(is.numeric) 
```

```{r}
ended_loan_new_num <- ended_loan_new_num[, colMeans(is.na(ended_loan_new_num)) < 0.000001]
```

```{r corrplot}
ended_loan_new_num[,-39] %>%
  cor() %>%
  corrplot::corrplot(method = "square", 
                     type = "full", 
                     order = "hclust",
                     tl.cex = 0.4, 
                     tl.col = "black",
                     na.label.col = "white")
```

To split the data

```{r, splitdata logic, warning=FALSE}
## add issue_d to the num data
ended_loan_new_num <- cbind(ended_loans_new$issue_d, ended_loan_new_num)
ended_loan_new_num <- ended_loan_new_num %>%
  rename(issue_d = `ended_loans_new$issue_d`)

## library(lubridate)
ended_loan_new_num$issue_d <- as.Date(gsub("^","01-", ended_loan_new_num$issue_d) ,format = "%d-%b-%Y")
ended_loan_new_num %>%
  group_by(terms, year(issue_d)) %>%
  summarise(count = n())
```

```{r split the data, warning=FALSE}
set.seed(654321)
TrainLoan <- ended_loan_new_num %>%
  filter(terms == 36 & year(issue_d) <= 2017 | 
           terms == 60 & year(issue_d) <= 2015)
TestLoan <- ended_loan_new_num %>%
  filter(terms == 36 & year(issue_d) > 2017 | 
           terms == 60 & year(issue_d) > 2015)
```

```{r}
TrainLoan <- TrainLoan %>%
  select(-terms, -issue_d)
TestLoan <- TestLoan %>%
  select(-terms, -issue_d)
```

```{r}
TrainLoan2 <- TrainLoan %>%
  select(-funded_amnt, -funded_amnt_inv, -total_pymnt, -total_pymnt_inv , 
         -total_rec_prncp , -tot_cur_bal , -total_rev_hi_lim, -last_pymnt_amnt,
         -total_rec_int, -status)

## set the Train to a smaller dataset n = 5849
trainIndex <- sample(nrow(TrainLoan2), as.integer(nrow(TrainLoan2) * 0.01))
TrainLoan3 <- TrainLoan2[trainIndex,]
TestLoan3 <- TrainLoan2[-trainIndex,]
```

```{r}
dim(TrainLoan3)
dim(TestLoan3)
```

```{R}
names(TrainLoan3)
```

```{r cross validation setup, warning=FALSE}
## setup 10 cross validation 
set.seed(654321)
train_Control <- trainControl(method = "cv",
                              number = 10)
```

### Linear modes
#### Linear regression

```{r lm, warning=FALSE}
## build the linear regression
##library(nlme)
lm.model <- train(ROI ~ ., 
                  data = TrainLoan3,
                  method = "lm",
                  trControl = train_Control,
                  na.action = na.exclude)
summary(lm.model)
```

```{r, warning=FALSE}
lm.model$result
```

#### Lasso

```{r Lasso, warning=FALSE}
## library(glmnet) ## for running glm lasso
enetGrid <- expand.grid(lambda = c(0, 0.01, 0.1),
                        fraction = seq(0.05, 1, length = 20))
set.seed(654321)
lasso_model <- train(x = TrainLoan3[,1:29],
                     y = TrainLoan3$ROI,
                     method = "enet",
                     tuneGrid = enetGrid,
                     trControl = train_Control,
                     preProcess = c("center", "scale"))

lasso_model
```

```{r, warning=FALSE}
## plot the Lasso
plot(lasso_model)
```

determine the correct complexity of the model

```{r, warning=FALSE}
lasso_model$results %>%
  arrange(RMSE) 
```


```{r, warning=FALSE}
ggplot(lasso_model$results, mapping = aes(x = fraction, y = RMSE, color = lambda)) +
  geom_pointrange(mapping = aes(ymin = RMSE - RMSESD,
                                ymax = RMSE + RMSESD)) +
  xlab("Number of fraction in Lasso Regression Model") +
  ylab("Cross Validation")
```


#### Ridge regresssion

```{r, warning=FALSE}
ridgeGrid <- expand.grid(lambda = seq(0, 0.1, length = 15))
set.seed(654321)
ridgeTune <- train(ROI~.,
                   data = TrainLoan3,
                   method = "ridge",
                   tuneGrid = ridgeGrid,
                   trControl = train_Control,
                   preProcess = c("center", "scale"))
ridgeTune$results %>%
  arrange(RMSE)
```

```{r, warning=FALSE}
print(update(plot(ridgeTune), xlab = "Penalty"))
```


### Trees 1. Regression tree

```{r reg tree, warning=FALSE}
library(rpart)
tree.reg <- rpart(ROI~., 
                  data = TrainLoan3)
tree.reg
```

plotting regression tree:   

```{r, warning=FALSE}
library(rpart.plot)
rpart.plot(tree.reg)
```

pruning `cp` value of the regression tree

```{r, warning=FALSE}
printcp(tree.reg)
```

```{r, warning=FALSE}
## visualize cross validation results
plotcp(tree.reg, upper = c("splits"))
```

```{r, warning=FALSE}
## prune tree and plot it
tree.reg.prune <- prune(tree.reg,
                        cp = tree.reg$cptable[which.min(tree.reg$cptable[,"xerror"]), "CP"])
rpart.plot(tree.reg.prune)
```


### Non-linear models

#### Multivariate Adaptive Regression Splines (MARS)

```{r MARS, warning=FALSE}
library(earth)
marsGrid <- expand.grid(degree = 1:3,
                        nprune = 1:29)
set.seed(654321)
marsTune <- train(ROI~.,
                  data = TrainLoan3,
                  method = "earth",
                  tuneGrid = marsGrid,
                  trControl = train_Control)
marsTune$results %>%
  arrange(RMSE)
```

plot the result of MARS

```{r, warning=FALSE}
plot(marsTune, 
     xlab = "nprune in MARS model")
```

MARS variable importance

```{r, warning=FALSE}
marsImp <- varImp(marsTune)
plot(marsImp)
```


#### K-nearest Neighbors (KNN)

```{r, warning=FALSE}
set.seed(654321)
knnTune <- train(ROI~.,
                 data = TrainLoan3,
                 method = "knn",
                 preProc = c("center", "scale"),
                 tuneGrid = data.frame(k = 1:20),
                 trControl = train_Control)
knnTune$results %>%
  arrange(RMSE)
```

```{r, warning=FALSE,}
plot(knnTune)
```

## Ensemble

#### Boosting 

```{r Boosting, warning=FALSE}
## setup
gbmGrid <- expand.grid(interaction.depth = seq(1, 7, by = 2),
                       n.trees = seq(200, 1000, by = 200),
                       shrinkage = c(0.01, 0.1),
                       n.minobsinnode = 10)
```

```{r, warning=FALSE}
## implement with caret
set.seed(654321)
gbmTune <- train(ROI~., 
                 data = TrainLoan3,
                 method = "gbm",
                 tuneGrid = gbmGrid,
                 trControl = train_Control,
                 verbose = FALSE)
gbmTune$results %>%
  arrange(RMSE)
```

Result plot of Boosting

```{r, warning=FALSE}
plot(gbmTune,
     auto.key = list(columns = 4, lines = TRUE))
```


```{r, warning=FALSE}
ggplot(gbmTune$results, mapping = aes(x = n.trees, y = RMSE,
                                      color = as.factor(interaction.depth))) +
  geom_pointrange(aes(ymin = RMSE - RMSESD,
                      ymax = RMSE + RMSESD)) +
  geom_line() +
  facet_grid(.~ as.ordered(shrinkage))
  
```

#### Bagging 

```{r Bagging, warning=FALSE}
set.seed(654321)
treebagTune <- train(ROI~., 
                     data = TrainLoan3,
                     method = "treebag",
                     nbagg = 100,
                     trControl = train_Control)
treebagTune
```

```{r, warning=FALSE}
treebagTune$results
```

## Results

```{r, warning=FALSE, echo=FALSE}
reflection <- data.frame(Model.Type = c("Linear","Lasso","RidgeRegression",
                                        "MARS", "KNN",
                                        "Boosting", "Bagging"),
                         RMSE = c(4755.518, 4760.536, 4764.022,
                                  4760.807, 4796.919,
                                  4742.001, 4764.729),
                         RMSESD = c(325.7274, 217.9801, 220.5997,
                                    222.3872, 267.8701,
                                    213.8701, 213.3656),
                         RSquared = c(0.0270844, 0.02435620, 0.02298867,
                                       0.03021918, 0.022483103,
                                       0.04093687, 0.02917148))
reflection
```

```{r, warning=FALSE, echo=FALSE}
ggplot(reflection, mapping = aes(x = Model.Type, y = RMSE)) +
  geom_pointrange(aes(ymin = RMSE - RMSESD,
                      ymax = RMSE + RMSESD)) +
  xlab("Model Type") +
  ylab("RMSE (Crossvalidation)") +
  geom_hline(aes(yintercept = min(RMSE)), 
             color = "tomato",
             linetype = "dashed", 
             size = 0.5)
```


## Predict

```{r}
library(gbm)
## par(mar = c(5, 8, 1, 1))
summary(gbmTune, 
        cBars = 15,
        method = relative.influence, # also can use permutation.test.gbm
        las = 2) 
```

```{r, predict}
prediction <- predict(gbmTune,
                      TestLoan3,
                      n.trees = 200) 
postResample(pred = prediction,
             obs = TestLoan3$ROI)
```

```{r}
predictcbind <- cbind(prediction, TestLoan3$ROI, TestLoan3$installment) 
predictcbind <- as.data.frame(predictcbind) %>%
  rename(TrueROI = V2, 
         installment = V3)
```

```{r comparison plot}
ggplot(predictcbind, mapping = aes(x = TrueROI)) +
  geom_density(color = "black", size = 1) +
  geom_vline(aes(xintercept = mean(TrueROI)),
             color = "black", 
             size = 1) +
  geom_density(aes(x = prediction), color = "tomato", size = 1) +
  geom_vline(aes(xintercept = mean(prediction)),
             color = "tomato",
             linetype = "dashed", 
             size = 1) +
  coord_cartesian(xlim=c(-1000, 1000)) +
  xlab("ROI")
```

```{r}
summary(predictcbind$prediction)
summary(predictcbind$TrueROI)
```

```{r}
ggplot(predictcbind) +
  geom_smooth(aes(x = installment, y = TrueROI), color = "gray") +
  geom_smooth(aes(x = installment, y = prediction), color = "tomato") +
  geom_smooth(aes(x = installment, y = prediction - TrueROI)) +
  ylab("ROI") 
```

```
{r}
save.image("CaseRegressionTidy_2.Rdata")
```




